{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06da5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf860b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo project\\classes\")\n",
    "from CNN_pytorch_ver import YoloDataset, YoloLoss, YOLOv1 #import my class CNN\n",
    "from CNN import CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f05753",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo project\\data\\Pascal_VOC\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfbab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo project\\data\\Pascal_VOC\\train\"\n",
    "\n",
    "xml_list = []\n",
    "for filename in os.listdir(path):\n",
    "    if not filename.endswith('.xml'): continue\n",
    "    fullname = os.path.join(path, filename)\n",
    "    xml_list.append(fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a4d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_CNN = CNN_model()\n",
    "my_CNN.get_annotation(xml_list)\n",
    "my_CNN.encode_pictures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5812d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo project\")\n",
    "my_CNN.df.to_json(r\"data\\annot_df.json\",orient=\"records\")\n",
    "torch.save(my_CNN.encoded_picture_annot, r\"data\\encoded_picture.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7885b271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>encoded_grid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13295</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13296</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13297</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13299</th>\n",
       "      <td>C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13299 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               file_name  \\\n",
       "1      C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "2      C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "3      C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "4      C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "5      C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "...                                                  ...   \n",
       "13295  C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "13296  C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "13297  C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "13298  C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "13299  C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo ...   \n",
       "\n",
       "                                            encoded_grid  \n",
       "1      [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "2      [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "3      [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "4      [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "5      [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "...                                                  ...  \n",
       "13295  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "13296  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "13297  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "13298  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "13299  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "\n",
       "[13299 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = torch.load(r\"data\\encoded_picture.pt\",weights_only=False)\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2c20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_encoded['file_name'],df_encoded['encoded_grid'],test_size=0.8)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169c36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = YoloDataset(X_train, y_train)\n",
    "val_dataset = YoloDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf64d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_11000\\1759946539.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Lucas\\Desktop\\vacantion classes\\Yolo project\\classes\\CNN_pytorch_ver.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.Y_tensor[idx], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_11000\\1759946539.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'YOLOv1' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11000\\1759946539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1739\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1741\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1748\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\vacantion classes\\Yolo project\\classes\\CNN_pytorch_ver.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1928\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m   1929\u001b[0m             \u001b[1;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'YOLOv1' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = YOLOv1(S=7, B=2, C=len(my_CNN.classes)).to(device)\n",
    "criterion = YoloLoss(S=7, B=2, C=len(my_CNN.classes), l_coord=5, l_noobj=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}/{len(train_loader)}\")\n",
    "\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Optional validation every few epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    preds = model(imgs)\n",
    "                    loss = criterion(preds, labels)\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095adcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"model/yolov1_weights.pth\")\n",
    "torch.save(model.state_dict(), \"model/yolov1_weights2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8814eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOv1(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU()\n",
       "    (27): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (28): ReLU()\n",
       "    (29): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (30): ReLU()\n",
       "    (31): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (32): ReLU()\n",
       "    (33): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): ReLU()\n",
       "    (35): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (36): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): ReLU()\n",
       "    (38): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (39): ReLU()\n",
       "    (40): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): ReLU()\n",
       "    (42): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (43): ReLU()\n",
       "    (44): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (45): ReLU()\n",
       "    (46): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (47): ReLU()\n",
       "    (48): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (49): ReLU()\n",
       "    (50): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (51): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=50176, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1470, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLOv1(S=7, B=2, C=len(my_CNN.classes))\n",
    "#model.load_state_dict(torch.load(\"model/yolov1_weights.pth\"))\n",
    "model.load_state_dict(torch.load(\"model/yolov1_weights2.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def display_predict(tensor, S, B, C, img_path, classes, score_threshold=0.3):\n",
    "    \"\"\"\n",
    "    tensor: PyTorch tensor of shape (S, S, B*5 + C) for a single image\n",
    "    img_path: path to the image\n",
    "    classes: list of class names\n",
    "    \"\"\"\n",
    "    # Ensure tensor is on CPU and NumPy\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        tensor = tensor.detach().cpu().numpy()\n",
    "    \n",
    "    boxes_list = [] \n",
    "    for i in range(S):\n",
    "        for j in range(S):\n",
    "            boxes = tensor[i,j, :B*5]           # bbox info: confidence + x + y + w + h\n",
    "            class_probs = tensor[i,j, B*5:]     # class probabilities\n",
    "            class_probs = 1 / (1 + np.exp(-class_probs)) # sigmoid to [0,1]\n",
    "\n",
    "            for b in range(B):\n",
    "                cls, x, y, w, h = boxes[b*5:(b+1)*5]\n",
    "                cls = 1 / (1 + np.exp(-cls))  # sigmoid confidence\n",
    "\n",
    "                img_h, img_w = 448, 448\n",
    "                x_img = (j + x) * img_w / S\n",
    "                y_img = (i + y) * img_h / S\n",
    "                w_img = w * img_w\n",
    "                h_img = abs(h) * img_h\n",
    "\n",
    "                class_id = np.argmax(class_probs)\n",
    "                class_prob = class_probs[class_id]\n",
    "                final_score = cls * class_prob\n",
    "\n",
    "                boxes_list.append({\n",
    "                    'cell': (i, j),\n",
    "                    'bbox': (x_img, y_img, w_img, h_img),\n",
    "                    'class_id': class_id,\n",
    "                    'score': final_score\n",
    "                })\n",
    "\n",
    "    # Filter by score threshold\n",
    "    boxes_list = [b for b in boxes_list if b['score'] > score_threshold]\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (448, 448))\n",
    "\n",
    "    for b in boxes_list:\n",
    "        x, y, w, h = b['bbox']\n",
    "        class_id = b['class_id']\n",
    "        score = b['score']\n",
    "        print(score)\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (int(x - w/2), int(y - h/2)), (int(x + w/2), int(y + h/2)), (0,255,0), 2)\n",
    "        cv2.putText(img, f'{classes[class_id]}:{score:.2f}', (int(x-w/2), int(y-h/2)-5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "\n",
    "    cv2.imshow('YOLO prediction', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.2059e-01,  5.3304e-01,  7.2433e-01,  ..., -5.6129e-03,\n",
      "           1.3125e-01, -3.4413e-02],\n",
      "         [ 4.3855e-01,  5.0437e-01,  6.7549e-01,  ..., -1.1587e-02,\n",
      "           1.4092e-01,  1.0947e-02],\n",
      "         [ 4.5433e-01,  5.1069e-01,  7.0333e-01,  ..., -4.1800e-04,\n",
      "           8.1996e-02,  6.3612e-03],\n",
      "         ...,\n",
      "         [ 3.6206e-01,  4.7363e-01,  6.5686e-01,  ..., -1.8519e-02,\n",
      "           8.1467e-02, -2.8356e-03],\n",
      "         [ 3.8179e-01,  5.4710e-01,  6.3997e-01,  ..., -6.0231e-03,\n",
      "           3.1887e-02, -4.9313e-03],\n",
      "         [ 4.0760e-01,  5.2286e-01,  6.7237e-01,  ..., -4.4671e-03,\n",
      "           8.0058e-02,  2.8121e-03]],\n",
      "\n",
      "        [[ 5.8473e-01,  5.6894e-01,  5.1745e-01,  ...,  9.7101e-03,\n",
      "           2.7937e-02, -4.2583e-03],\n",
      "         [ 5.9542e-01,  5.0293e-01,  5.4507e-01,  ..., -1.9378e-03,\n",
      "           9.4919e-02, -3.0420e-03],\n",
      "         [ 6.5462e-01,  4.5947e-01,  6.1109e-01,  ..., -7.3028e-04,\n",
      "           9.7591e-02,  4.0041e-02],\n",
      "         ...,\n",
      "         [ 6.2964e-01,  5.1596e-01,  5.9989e-01,  ...,  3.0322e-03,\n",
      "           7.9553e-02,  6.1627e-02],\n",
      "         [ 5.7810e-01,  5.1397e-01,  5.3857e-01,  ...,  8.1564e-03,\n",
      "           9.4509e-02,  3.2933e-02],\n",
      "         [ 5.8544e-01,  4.0655e-01,  5.6466e-01,  ..., -8.2517e-03,\n",
      "           7.0065e-02,  4.5807e-03]],\n",
      "\n",
      "        [[ 7.2807e-01,  5.3597e-01,  5.7647e-01,  ...,  3.4692e-02,\n",
      "           3.6924e-02, -1.5435e-03],\n",
      "         [ 8.1725e-01,  4.9192e-01,  5.7247e-01,  ...,  8.3140e-03,\n",
      "           6.6449e-03,  8.3822e-04],\n",
      "         [ 8.6682e-01,  5.5080e-01,  5.8123e-01,  ...,  2.5731e-03,\n",
      "           2.4396e-02,  3.3791e-02],\n",
      "         ...,\n",
      "         [ 8.2715e-01,  4.5795e-01,  6.1486e-01,  ...,  2.1427e-02,\n",
      "           2.0825e-02,  4.2855e-02],\n",
      "         [ 8.0744e-01,  5.0088e-01,  5.0748e-01,  ...,  3.9431e-02,\n",
      "           4.1751e-02,  3.3601e-02],\n",
      "         [ 7.0585e-01,  4.0366e-01,  5.9395e-01,  ...,  6.0536e-02,\n",
      "           9.5491e-02,  5.6680e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.9699e-01,  6.6204e-01,  4.5474e-01,  ...,  5.4100e-02,\n",
      "           4.1353e-02, -3.2236e-03],\n",
      "         [ 8.8845e-01,  5.4551e-01,  4.2261e-01,  ...,  2.7038e-02,\n",
      "           1.8633e-02,  4.0323e-02],\n",
      "         [ 9.5428e-01,  4.7656e-01,  3.8987e-01,  ..., -8.6232e-03,\n",
      "          -3.5186e-03,  6.3256e-02],\n",
      "         ...,\n",
      "         [ 9.2984e-01,  4.4279e-01,  4.4011e-01,  ...,  1.7203e-02,\n",
      "           1.9407e-03,  2.5885e-02],\n",
      "         [ 9.1598e-01,  4.5154e-01,  4.7381e-01,  ...,  1.1755e-03,\n",
      "           1.0208e-02,  3.0990e-02],\n",
      "         [ 8.0571e-01,  4.2633e-01,  4.1931e-01,  ...,  3.6461e-03,\n",
      "           2.1139e-02, -5.0499e-04]],\n",
      "\n",
      "        [[ 7.2140e-01,  6.1411e-01,  4.5689e-01,  ...,  4.3855e-03,\n",
      "           3.3406e-02, -5.7475e-03],\n",
      "         [ 7.7769e-01,  5.0125e-01,  4.5198e-01,  ..., -2.8447e-04,\n",
      "           3.3935e-02,  7.9076e-03],\n",
      "         [ 8.2949e-01,  5.7793e-01,  4.4863e-01,  ...,  1.0750e-02,\n",
      "           8.0846e-03,  5.0499e-02],\n",
      "         ...,\n",
      "         [ 7.8799e-01,  4.6848e-01,  4.0833e-01,  ...,  3.4965e-02,\n",
      "           3.2759e-02,  5.0913e-02],\n",
      "         [ 7.6130e-01,  4.6528e-01,  4.1350e-01,  ..., -2.6491e-03,\n",
      "           3.8450e-02, -3.1166e-03],\n",
      "         [ 6.8174e-01,  3.9364e-01,  5.1183e-01,  ..., -7.3076e-03,\n",
      "           2.2133e-02, -3.9366e-03]],\n",
      "\n",
      "        [[ 3.3876e-01,  5.8544e-01,  3.1454e-01,  ..., -3.7495e-03,\n",
      "           1.7718e-02,  8.4369e-03],\n",
      "         [ 4.1171e-01,  5.0279e-01,  3.3515e-01,  ...,  4.0405e-03,\n",
      "           1.1902e-01, -1.9385e-03],\n",
      "         [ 4.3303e-01,  4.8853e-01,  2.3598e-01,  ...,  5.8213e-02,\n",
      "           2.5488e-02,  4.9500e-02],\n",
      "         ...,\n",
      "         [ 4.0153e-01,  5.5442e-01,  2.4134e-01,  ...,  3.8478e-03,\n",
      "           2.5937e-02,  4.7651e-03],\n",
      "         [ 3.5448e-01,  4.9828e-01,  2.1832e-01,  ..., -5.0593e-03,\n",
      "           2.7712e-02, -2.0847e-03],\n",
      "         [ 4.2130e-01,  4.0606e-01,  2.5660e-01,  ..., -2.2665e-02,\n",
      "           5.2326e-02,  5.3404e-03]]], device='cuda:0')\n",
      "0.43731487436091\n",
      "0.43618338762428255\n",
      "0.44317982507654713\n",
      "0.4448206089636359\n",
      "0.45057061962481243\n",
      "0.4429586362120362\n",
      "0.4469867275287424\n",
      "0.45471103404367297\n",
      "0.44370996467480034\n",
      "0.45119150141957853\n",
      "0.44737557690373575\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():            \n",
    "    img = cv2.imread(X_test[12])\n",
    "    img = cv2.resize(img, (448, 448))\n",
    "    img = img[..., ::-1]  # BGR to RGB\n",
    "    img = img / 255.0\n",
    "    sample_img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # (C,H,W)\n",
    "    sample_img_tensor = sample_img.unsqueeze(0).to(device)  # add batch dimension\n",
    "    pred = model(sample_img_tensor)             # shape: (1, S, S, B*5 + C)\n",
    "    pred = pred[0]    \n",
    "    print(pred)                          # remove batch dimension\n",
    "    display_predict(pred, S=7, B=2, C=len(my_CNN.classes), img_path=X_test[24], classes=my_CNN.classes, score_threshold=0.43)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
